{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#import evaluate\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    l = []\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        l.append(subtree.leaves())\n",
    "    \n",
    "    return l\n",
    "\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    #word = stemmer.stem(word)\n",
    "    #word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "\n",
    "def get_terms(tree):\n",
    "    kp = []\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        if term:\n",
    "            kp.append(term)\n",
    "    \n",
    "    return kp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_kp(text):\n",
    "    \n",
    "    # Used when tokenizing words\n",
    "    sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "          | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "          | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "          | \\.\\.\\.              # ellipsis\n",
    "          | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "        '''\n",
    "\n",
    "    #lemmatizer = nltk.WordNetLemmatizer()\n",
    "    #stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "    #Taken from Su Nam Kim Paper\n",
    "    grammar = r\"\"\"\n",
    "        NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "        NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "\n",
    "    #toks = nltk.regexp_tokenize(text, sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(text)\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tree = chunker.parse(postoks)\n",
    "    terms = get_terms(tree)\n",
    "    #pos,pos_set = find_positions(text,terms)\n",
    "    return terms#,pos,pos_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Inspec/docsutf8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = 'Inspec/keys/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(data)\n",
    "key_files = os.listdir(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(document,bert_tocs, kps):\n",
    "    ''' \n",
    "    merge the same kps & keep present kps in document\n",
    "    Inputs:\n",
    "        document : a word list : ['sun', 'sunshine', ...] || lower cased\n",
    "        kps : can have more than one kp : [['sun'], ['key','phrase'], ['sunshine']] || not duplicate\n",
    "    Outputs:\n",
    "        all_present_kps : present keyphrases\n",
    "        positions_for_all : start_end_posisiton for prensent keyphrases\n",
    "        a present kp postions list : every present's positions in documents, \n",
    "        each kp can be presented in several postions .\n",
    "        [[[0,0],[20,21]], [[1,1]]]\n",
    "    '''\n",
    "    tot_doc_char = ' '.join(document)\n",
    "    \n",
    "    positions_for_all = []\n",
    "    position_start,position_end =[],[]\n",
    "    all_present_kps = []\n",
    "    for kp in kps:\n",
    "        ans_string = ' '.join(kp)\n",
    "        \n",
    "        if ans_string not in tot_doc_char:\n",
    "            continue\n",
    "        else: \n",
    "            positions_for_each = []\n",
    "            # find all positions for each kp\n",
    "            for i in range(0, len(bert_tocs) - len(kp) + 1):\n",
    "                found = False\n",
    "                search_str = ''\n",
    "                if ans_string.startswith(bert_tocs[i]):\n",
    "                    found = True\n",
    "                    search_str +=bert_tocs[i]\n",
    "                    search_idx = i\n",
    "                    while found and search_idx<(len(bert_tocs)-1):\n",
    "                        search_idx+=1\n",
    "                        if search_str+bert_tocs[search_idx] in ans_string:\n",
    "                            search_str+=bert_tocs[search_idx]\n",
    "                        elif search_str+' '+bert_tocs[search_idx] in ans_string:\n",
    "                            search_str+=' '+bert_tocs[search_idx]\n",
    "                        else:\n",
    "                            found = False\n",
    "                        \n",
    "                if (search_str==ans_string) and (i<search_idx):\n",
    "                    assert len(kp) >= 1\n",
    "                    positions_for_each.append((i+1, search_idx))\n",
    "                    position_start.append(i+1)\n",
    "                    position_end.append(search_idx)\n",
    "                    \n",
    "        if len(positions_for_each) > 0 :\n",
    "            positions_for_all.extend(positions_for_each)\n",
    "            all_present_kps.append(kp)\n",
    "           \n",
    "    assert len(positions_for_all) >= len(all_present_kps)\n",
    "    \n",
    "    if len(all_present_kps) == 0:\n",
    "        return [None,None]\n",
    "    return [position_start,position_end],set(positions_for_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in string.punctuation]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text=[word for word in text if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(y_labels,y_preds,depth,levels):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for idx,y_label in enumerate(y_labels):\n",
    "        tp = 0\n",
    "        p = []\n",
    "        r = []\n",
    "        y_label = set(np.where(y_label==1)[0])\n",
    "        #print(y_preds[idx].shape)\n",
    "        key_idx = np.argsort(y_preds[idx])#[:,0])\n",
    "        #print(sorted(-y_preds[idx]))\n",
    "        y_new = np.sort(y_preds[idx])[::-1]\n",
    "        #print(y_preds[idx])\n",
    "        preds = key_idx[y_new>=0.5]\n",
    "        for i in range(depth):\n",
    "            if len(preds)>i:\n",
    "                if preds[i] in y_label:\n",
    "                    tp+=1\n",
    "            p.append(tp/(min(i,len(preds))+1))\n",
    "            r.append(tp/max(len(y_label),1))\n",
    "    \n",
    "    \n",
    "        level_index = []\n",
    "        level_p = []\n",
    "        level_r = []\n",
    "        for idx,level in enumerate(levels):\n",
    "            level_p.append(p[level-1])\n",
    "            level_r.append(r[level-1])\n",
    "            if p[level-1]+r[level-1]>0:\n",
    "                level_index.append(2*p[level-1]*r[level-1]/(p[level-1]+r[level-1]))\n",
    "            else:\n",
    "                level_index.append(0)\n",
    "        #print('k',level_index)\n",
    "        precision.append(level_p)\n",
    "        recall.append(level_r)\n",
    "        f1.append(level_index)\n",
    "    precision = np.array(precision)\n",
    "    recall = np.array(recall)\n",
    "    f1 = np.array(f1)\n",
    "    \n",
    "    print('F1',np.mean(f1,axis=0),np.mean(precision,axis=0),np.mean(recall,axis=0))\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "candidates = []\n",
    "references = []\n",
    "\n",
    "for file in files[:20]:\n",
    "    with open(data+file, 'r') as in_file:        \n",
    "        text = in_file.read()\n",
    "        candidates.append({'url':file,\n",
    "                            'KeyPhrases':get_kp(text)})\n",
    "    \n",
    "    name = file.split('.')[0]\n",
    "    with open(keys+name+'.key', 'r') as in_file:\n",
    "\n",
    "        can = in_file.readlines()\n",
    "        can = [line.rstrip('\\n').split() for line in can]\n",
    "        references.append({'url':file,\n",
    "                            'KeyPhrases':can})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "with open('result.json', 'w') as out_file:\n",
    "    for candidate in candidates:\n",
    "        json.dump(candidate, out_file)\n",
    "        out_file.write('\\n')\n",
    "with open('keys.json', 'w') as out_file:\n",
    "    for ref in references:\n",
    "        json.dump(ref, out_file)\n",
    "        out_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "vocab = \"D:/Word embedding/bert/assets/vocab.txt\"\n",
    "tokenizer = BertWordPieceTokenizer(vocab, lowercase=True)\n",
    "encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 1e+100\n"
     ]
    }
   ],
   "source": [
    "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "max_kp = 0\n",
    "min_len =1e100\n",
    "all_reps = []\n",
    "att_masks = []\n",
    "key_positions = []\n",
    "ref_positions = []\n",
    "vocab = \"D:/Word embedding/bert/assets/vocab.txt\"\n",
    "tokenizer = BertWordPieceTokenizer(vocab, lowercase=True)\n",
    "max_len = 512\n",
    "for file in files[:1600]:\n",
    "    with open(data+file, 'r') as in_file:\n",
    "        text = in_file.read()\n",
    "        txt = remove_punctuation(text)\n",
    "        text_toc = nltk.regexp_tokenize(txt, sentence_re)\n",
    "        bert_toc_idx = tokenizer.encode(txt)\n",
    "        bert_tocs = [txt[start:end] for start,end in bert_toc_idx.offsets]\n",
    "        padding_length = max_len - len(bert_tocs)\n",
    "        rep = bert_toc_idx.ids\n",
    "        \n",
    "        if padding_length<0:\n",
    "            rep = np.array(rep[:max_len])\n",
    "            attention_mask = np.array([1]*max_len)\n",
    "        else:\n",
    "            attention_mask = np.array(([1]*len(rep))+([0]*padding_length))\n",
    "            rep = np.array(rep + ([0]*padding_length))\n",
    "            \n",
    "         \n",
    "        kps = get_kp(text_toc)\n",
    "        pos,pos_set = find_positions(text_toc,bert_tocs,kps)\n",
    "    \n",
    "    name = file.split('.')[0]\n",
    "    with open(keys+name+'.key', 'r') as in_file:\n",
    "        if pos_set:\n",
    "            can = in_file.readlines()\n",
    "            can = [line.rstrip('\\n').split() for line in can]\n",
    "            ref_pos,ref_set = find_positions(text_toc,bert_tocs,can)\n",
    "            \n",
    "        if pos_set and ref_set:\n",
    "            key_positions.append(pos_set)\n",
    "            #rep = np.zeros((len(text_toc),300),dtype=float)\n",
    "            #rep[idx] = model2[np.array(text_toc)[idx]]  \n",
    "            all_reps.append(rep)\n",
    "            att_masks.append(attention_mask)\n",
    "            #max_len = max(max_len,len(text_toc))\n",
    "            #min_len = min(min_len,len(text_toc))\n",
    "            max_kp = max(max_kp,len(pos_set))\n",
    "            ref_positions.append(ref_set)\n",
    "            \n",
    "print(max_len,min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_label = []\n",
    "\n",
    "final_positions = []\n",
    "pos_mask = []\n",
    "final_kp_list = []\n",
    "for idx,kp in enumerate(key_positions):\n",
    "    kp = list(kp)\n",
    "    start = []\n",
    "    end = []\n",
    "    y_val = []\n",
    "    #y_val = [2 if (key in ref_positions[idx] and key[1]<512) else 1 for key in kp]\n",
    "    for key in kp:\n",
    "        if key[1]<512:\n",
    "            start.append(key[0]-1)\n",
    "            end.append(key[1]-1)\n",
    "            y_val.append([2,1] if key in ref_positions[idx] else [1,2])\n",
    "            \n",
    "    y_val.extend([[0,0]]*(max_kp-len(start)))\n",
    "    final_positions.append(tf.pad([start,end],[[0,0],[0,max_kp-len(start)]]))\n",
    "    pos_mask.append([True]*len(start)+[False]*(max_kp-len(start)))\n",
    "    \n",
    "    y_label.append(y_val)\n",
    "    final_kp_list.append(kp)\n",
    "    #print(idx,len(y_val),max_kp-len(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_label[164])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = tf.transpose(all_reps,perm=[0,1])\n",
    "x_mask = tf.transpose(att_masks,perm=[0,1])\n",
    "x_pos = tf.stack(final_positions)\n",
    "#x_pos = tf.tile(tf.expand_dims(x_pos,-1),[1,1,1,100])\n",
    "y_train = tf.stack(y_label)\n",
    "y_train = tf.cast(y_train,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1588, 146, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,) (512,) (1588, 146, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape,x_train[1].shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "embedding = encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "bilstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24,\n",
    "                                                             #kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.02,stddev=0.25),\n",
    "                                                             dropout = 0.35,\n",
    "                                                             return_sequences=True),\n",
    "                                                             merge_mode=None)(embedding)\n",
    "pos_mask = tf.keras.layers.Input(shape=(2,146),dtype='int32')\n",
    "mask_start = pos_mask[0][0]\n",
    "mask_end = pos_mask[0][1]\n",
    "\n",
    "start_rep_fr = tf.gather(bilstm1[0],mask_start,axis=1)\n",
    "start_rep_bk = tf.gather(bilstm1[1],mask_start,axis=1)\n",
    "end_rep_fr = tf.gather(bilstm1[0],mask_end,axis=1)\n",
    "end_rep_bk = tf.gather(bilstm1[0],mask_end,axis=1)\n",
    "\n",
    "\n",
    "span_fe_diff_fr = start_rep_fr-end_rep_fr\n",
    "span_fe_prod_fr = tf.math.multiply(start_rep_fr,end_rep_fr)\n",
    "span_fe_diff_bk = start_rep_bk-end_rep_bk\n",
    "span_fe_prod_bk = tf.math.multiply(start_rep_bk,end_rep_bk)\n",
    "\n",
    "\n",
    "span_fe = tf.keras.layers.concatenate([start_rep_fr,\n",
    "                     end_rep_fr,\n",
    "                     start_rep_bk,\n",
    "                     end_rep_bk,\n",
    "                     span_fe_diff_fr,\n",
    "                     span_fe_diff_bk,\n",
    "                     span_fe_prod_fr,\n",
    "                     span_fe_prod_bk\n",
    "                    ],2)\n",
    "bilstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(4,return_sequences=True,dropout = 0.15,\n",
    "                                                            #kernel_initializer=tf.keras.initializers.(mean=0.0,stddev=0.05),\n",
    "                                                            ),\n",
    "                                        \n",
    "                                         merge_mode='ave',\n",
    "                                         input_shape=(146,16*4))(span_fe)\n",
    "output = tf.keras.layers.Dense(2,activation='softmax')(bilstm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kpe_model = tf.keras.models.Model(inputs=[input_ids,attention_mask,pos_mask], outputs=output)\n",
    "kpe_model.layers[3].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 2, 146)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(2, 146)]           0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(2, 146)]           0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 512, 24), (N 152256      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(146,)]             0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(146,)]             0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2 (TensorFlo [(None, 146, 24)]    0           bidirectional[0][0]              \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2_2 (TensorF [(None, 146, 24)]    0           bidirectional[0][0]              \n",
      "                                                                 tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2_1 (TensorF [(None, 146, 24)]    0           bidirectional[0][1]              \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_GatherV2_3 (TensorF [(None, 146, 24)]    0           bidirectional[0][0]              \n",
      "                                                                 tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 146, 24)]    0           tf_op_layer_GatherV2[0][0]       \n",
      "                                                                 tf_op_layer_GatherV2_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(None, 146, 24)]    0           tf_op_layer_GatherV2_1[0][0]     \n",
      "                                                                 tf_op_layer_GatherV2_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 146, 24)]    0           tf_op_layer_GatherV2[0][0]       \n",
      "                                                                 tf_op_layer_GatherV2_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [(None, 146, 24)]    0           tf_op_layer_GatherV2_1[0][0]     \n",
      "                                                                 tf_op_layer_GatherV2_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 146, 192)     0           tf_op_layer_GatherV2[0][0]       \n",
      "                                                                 tf_op_layer_GatherV2_2[0][0]     \n",
      "                                                                 tf_op_layer_GatherV2_1[0][0]     \n",
      "                                                                 tf_op_layer_GatherV2_3[0][0]     \n",
      "                                                                 tf_op_layer_Sub[0][0]            \n",
      "                                                                 tf_op_layer_Sub_1[0][0]          \n",
      "                                                                 tf_op_layer_Mul[0][0]            \n",
      "                                                                 tf_op_layer_Mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 146, 4)       6304        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 146, 2)       10          bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 109,640,810\n",
      "Trainable params: 158,570\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(kpe_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    #print(y_pred.shape,y_true.shape)\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    y_true = tf.clip_by_value(y_true-1, 0, 1)\n",
    "    loss_ = loss_object(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    #loss_ = tf.boolean_mask(loss_,mask)\n",
    "    \n",
    "    return tf.reduce_mean(loss_)#tf.reduce_mean(tf.reduce_sum(loss_,axis=1)/tf.reduce_sum(mask,axis=1))\n",
    "\n",
    "def ac_metrics(y_true,y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    y_true = tf.cast(tf.clip_by_value(y_true-1, 0, 1),dtype='int32')\n",
    "    y_pred = tf.cast(tf.where(y_pred>=0.5,1,0),dtype='int32')\n",
    "    diff = 1-tf.abs(y_true-y_pred)\n",
    "    mask = tf.cast(mask, dtype=diff.dtype)\n",
    "    diff*= mask\n",
    "    \n",
    "    return tf.reduce_mean(tf.reduce_sum(diff,axis=1)/tf.reduce_sum(mask,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.000003)\n",
    "kpe_model.compile(optimizer=opt,\n",
    "              loss=loss_function,\n",
    "              metrics=[ac_metrics])\n",
    "\n",
    "#checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_ac_metrics:.2f}.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_ac_metrics', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0343 - ac_metrics: 0.17 - ETA: 58s - loss: 0.0355 - ac_metrics: 0.170 - ETA: 1:16 - loss: 0.0365 - ac_metrics: 0.17 - ETA: 1:23 - loss: 0.0339 - ac_metrics: 0.18 - ETA: 1:25 - loss: 0.0358 - ac_metrics: 0.17 - ETA: 1:26 - loss: 0.0377 - ac_metrics: 0.18 - ETA: 1:26 - loss: 0.0362 - ac_metrics: 0.19 - ETA: 1:25 - loss: 0.0357 - ac_metrics: 0.18 - ETA: 1:23 - loss: 0.0352 - ac_metrics: 0.18 - ETA: 1:21 - loss: 0.0344 - ac_metrics: 0.18 - ETA: 1:19 - loss: 0.0349 - ac_metrics: 0.18 - ETA: 1:17 - loss: 0.0360 - ac_metrics: 0.18 - ETA: 1:14 - loss: 0.0364 - ac_metrics: 0.18 - ETA: 1:11 - loss: 0.0363 - ac_metrics: 0.18 - ETA: 1:09 - loss: 0.0364 - ac_metrics: 0.19 - ETA: 1:06 - loss: 0.0363 - ac_metrics: 0.19 - ETA: 1:03 - loss: 0.0364 - ac_metrics: 0.19 - ETA: 1:00 - loss: 0.0364 - ac_metrics: 0.19 - ETA: 57s - loss: 0.0368 - ac_metrics: 0.1925 - ETA: 54s - loss: 0.0366 - ac_metrics: 0.190 - ETA: 51s - loss: 0.0368 - ac_metrics: 0.189 - ETA: 48s - loss: 0.0365 - ac_metrics: 0.189 - ETA: 45s - loss: 0.0362 - ac_metrics: 0.187 - ETA: 41s - loss: 0.0368 - ac_metrics: 0.186 - ETA: 38s - loss: 0.0369 - ac_metrics: 0.187 - ETA: 35s - loss: 0.0369 - ac_metrics: 0.185 - ETA: 32s - loss: 0.0369 - ac_metrics: 0.186 - ETA: 29s - loss: 0.0368 - ac_metrics: 0.186 - ETA: 26s - loss: 0.0365 - ac_metrics: 0.187 - ETA: 22s - loss: 0.0365 - ac_metrics: 0.186 - ETA: 19s - loss: 0.0360 - ac_metrics: 0.186 - ETA: 16s - loss: 0.0361 - ac_metrics: 0.186 - ETA: 13s - loss: 0.0362 - ac_metrics: 0.185 - ETA: 9s - loss: 0.0362 - ac_metrics: 0.185 - ETA: 6s - loss: 0.0362 - ac_metrics: 0.18 - ETA: 3s - loss: 0.0362 - ac_metrics: 0.18 - ETA: 0s - loss: 0.0363 - ac_metrics: 0.18 - 134s 4s/step - loss: 0.0363 - ac_metrics: 0.1852 - val_loss: 0.0216 - val_ac_metrics: 0.1980\n",
      "Epoch 2/6\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0393 - ac_metrics: 0.20 - ETA: 58s - loss: 0.0365 - ac_metrics: 0.198 - ETA: 1:16 - loss: 0.0358 - ac_metrics: 0.19 - ETA: 1:23 - loss: 0.0349 - ac_metrics: 0.19 - ETA: 1:26 - loss: 0.0336 - ac_metrics: 0.19 - ETA: 1:27 - loss: 0.0344 - ac_metrics: 0.19 - ETA: 1:26 - loss: 0.0361 - ac_metrics: 0.19 - ETA: 1:25 - loss: 0.0372 - ac_metrics: 0.19 - ETA: 1:23 - loss: 0.0373 - ac_metrics: 0.19 - ETA: 1:21 - loss: 0.0368 - ac_metrics: 0.18 - ETA: 1:19 - loss: 0.0368 - ac_metrics: 0.19 - ETA: 1:17 - loss: 0.0360 - ac_metrics: 0.19 - ETA: 1:14 - loss: 0.0362 - ac_metrics: 0.18 - ETA: 1:11 - loss: 0.0352 - ac_metrics: 0.18 - ETA: 1:09 - loss: 0.0348 - ac_metrics: 0.18 - ETA: 1:06 - loss: 0.0356 - ac_metrics: 0.18 - ETA: 1:03 - loss: 0.0360 - ac_metrics: 0.18 - ETA: 1:00 - loss: 0.0366 - ac_metrics: 0.18 - ETA: 57s - loss: 0.0366 - ac_metrics: 0.1851 - ETA: 54s - loss: 0.0363 - ac_metrics: 0.186 - ETA: 51s - loss: 0.0360 - ac_metrics: 0.186 - ETA: 48s - loss: 0.0358 - ac_metrics: 0.187 - ETA: 45s - loss: 0.0360 - ac_metrics: 0.187 - ETA: 41s - loss: 0.0354 - ac_metrics: 0.189 - ETA: 38s - loss: 0.0354 - ac_metrics: 0.191 - ETA: 35s - loss: 0.0352 - ac_metrics: 0.191 - ETA: 32s - loss: 0.0360 - ac_metrics: 0.190 - ETA: 29s - loss: 0.0360 - ac_metrics: 0.191 - ETA: 26s - loss: 0.0362 - ac_metrics: 0.191 - ETA: 22s - loss: 0.0365 - ac_metrics: 0.190 - ETA: 19s - loss: 0.0364 - ac_metrics: 0.189 - ETA: 16s - loss: 0.0361 - ac_metrics: 0.191 - ETA: 13s - loss: 0.0361 - ac_metrics: 0.190 - ETA: 9s - loss: 0.0363 - ac_metrics: 0.190 - ETA: 6s - loss: 0.0364 - ac_metrics: 0.18 - ETA: 3s - loss: 0.0361 - ac_metrics: 0.18 - ETA: 0s - loss: 0.0360 - ac_metrics: 0.19 - 131s 4s/step - loss: 0.0360 - ac_metrics: 0.1905 - val_loss: 0.0213 - val_ac_metrics: 0.2048\n",
      "Epoch 3/6\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0355 - ac_metrics: 0.23 - ETA: 1:00 - loss: 0.0352 - ac_metrics: 0.21 - ETA: 1:17 - loss: 0.0376 - ac_metrics: 0.20 - ETA: 1:24 - loss: 0.0376 - ac_metrics: 0.19 - ETA: 1:27 - loss: 0.0386 - ac_metrics: 0.20 - ETA: 1:28 - loss: 0.0387 - ac_metrics: 0.20 - ETA: 1:28 - loss: 0.0371 - ac_metrics: 0.20 - ETA: 1:26 - loss: 0.0381 - ac_metrics: 0.20 - ETA: 1:25 - loss: 0.0375 - ac_metrics: 0.20 - ETA: 1:23 - loss: 0.0378 - ac_metrics: 0.19 - ETA: 1:20 - loss: 0.0375 - ac_metrics: 0.19 - ETA: 1:18 - loss: 0.0380 - ac_metrics: 0.19 - ETA: 1:15 - loss: 0.0376 - ac_metrics: 0.19 - ETA: 1:13 - loss: 0.0372 - ac_metrics: 0.19 - ETA: 1:10 - loss: 0.0374 - ac_metrics: 0.19 - ETA: 1:07 - loss: 0.0379 - ac_metrics: 0.19 - ETA: 1:04 - loss: 0.0377 - ac_metrics: 0.19 - ETA: 1:01 - loss: 0.0380 - ac_metrics: 0.20 - ETA: 58s - loss: 0.0377 - ac_metrics: 0.2000 - ETA: 55s - loss: 0.0384 - ac_metrics: 0.198 - ETA: 52s - loss: 0.0380 - ac_metrics: 0.199 - ETA: 49s - loss: 0.0381 - ac_metrics: 0.199 - ETA: 45s - loss: 0.0380 - ac_metrics: 0.199 - ETA: 42s - loss: 0.0377 - ac_metrics: 0.201 - ETA: 39s - loss: 0.0374 - ac_metrics: 0.202 - ETA: 36s - loss: 0.0372 - ac_metrics: 0.203 - ETA: 32s - loss: 0.0377 - ac_metrics: 0.206 - ETA: 29s - loss: 0.0370 - ac_metrics: 0.206 - ETA: 26s - loss: 0.0368 - ac_metrics: 0.207 - ETA: 23s - loss: 0.0367 - ac_metrics: 0.207 - ETA: 19s - loss: 0.0367 - ac_metrics: 0.207 - ETA: 16s - loss: 0.0366 - ac_metrics: 0.207 - ETA: 13s - loss: 0.0360 - ac_metrics: 0.208 - ETA: 9s - loss: 0.0356 - ac_metrics: 0.208 - ETA: 6s - loss: 0.0354 - ac_metrics: 0.20 - ETA: 3s - loss: 0.0353 - ac_metrics: 0.20 - ETA: 0s - loss: 0.0355 - ac_metrics: 0.20 - 133s 4s/step - loss: 0.0355 - ac_metrics: 0.2069 - val_loss: 0.0209 - val_ac_metrics: 0.2194\n",
      "Epoch 4/6\n",
      "12/37 [========>.....................] - ETA: 0s - loss: 0.0363 - ac_metrics: 0.24 - ETA: 59s - loss: 0.0372 - ac_metrics: 0.225 - ETA: 1:17 - loss: 0.0335 - ac_metrics: 0.23 - ETA: 1:24 - loss: 0.0356 - ac_metrics: 0.23 - ETA: 1:27 - loss: 0.0344 - ac_metrics: 0.21 - ETA: 1:28 - loss: 0.0331 - ac_metrics: 0.23 - ETA: 1:27 - loss: 0.0320 - ac_metrics: 0.24 - ETA: 1:26 - loss: 0.0325 - ac_metrics: 0.24 - ETA: 1:25 - loss: 0.0317 - ac_metrics: 0.24 - ETA: 1:23 - loss: 0.0322 - ac_metrics: 0.24 - ETA: 1:20 - loss: 0.0317 - ac_metrics: 0.24 - ETA: 1:18 - loss: 0.0326 - ac_metrics: 0.2358"
     ]
    }
   ],
   "source": [
    "history = kpe_model.fit([x_train[:1300],x_mask[:1300],x_pos[:1300]], y_train[:1300], \n",
    "                          batch_size=32,epochs=6,#callbacks=callbacks_list, \n",
    "                          use_multiprocessing=True,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['ac_metrics'])\n",
    "plt.plot(history.history['val_ac_metrics'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for test\")\n",
    "predictions = kpe_model.predict([x_train[1300:],x_mask[1300:],x_pos[1300:]])\n",
    "print(\"predictions shape:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_f1(np.where(y_train[1300:,:,0]-1==1,1,0),predictions[:,:,0],20,[5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(predictions[101,:,0][np.where((y_train[1101,:,0])>0)]>=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions[40,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((y_train[1101,:,0]-1)==1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
