{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import extractor as ext\n",
    "from utils import extractionUtils as extUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_kps(doc,model,mode='inspec'):\n",
    "    if mode=='inspec':\n",
    "        kp_lim = 154\n",
    "    else:\n",
    "        kp_lim = 258\n",
    "    sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "          | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "          | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "          | \\.\\.\\.              # ellipsis\n",
    "          | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "        '''   \n",
    "    txt = extUtil.remove_punctuation(doc)\n",
    "    text_toc = nltk.regexp_tokenize(txt, sentence_re)\n",
    "    bert_toc_idx = tokenizer.encode(txt)\n",
    "    bert_tocs = [txt[start:end] for start,end in bert_toc_idx.offsets]\n",
    "    padding_length = max_len - len(bert_tocs)\n",
    "    rep = bert_toc_idx.ids\n",
    "\n",
    "    if padding_length<0:\n",
    "        rep = np.array(rep[:max_len])\n",
    "        attention_mask = np.array([1]*max_len)\n",
    "    else:\n",
    "        attention_mask = np.array(([1]*len(rep))+([0]*padding_length))\n",
    "        rep = np.array(rep + ([0]*padding_length))\n",
    "\n",
    "\n",
    "    kps = extUtil.get_kp(text_toc)\n",
    "    pos,pos_set = extUtil.find_positions(text_toc,bert_tocs,kps)\n",
    "    kp = list(pos_set)\n",
    "    start = []\n",
    "    end = []\n",
    "    \n",
    "    #y_val = [2 if (key in ref_positions[idx] and key[1]<512) else 1 for key in kp]\n",
    "    for key in kp:\n",
    "        if key[1]<512:\n",
    "            start.append(key[0]-1)\n",
    "            end.append(key[1]-1)\n",
    "            \n",
    "            \n",
    "    x_pos = tf.pad([start,end],[[0,0],[0,max_kp-len(start)]])\n",
    "    x_pred = tf.expand_dims(rep,0)\n",
    "    \n",
    "    x_pred_mask = tf.expand_dims(attention_mask,0)\n",
    "    x_pred_pos = tf.expand_dims(tf.stack(x_pos),0)\n",
    "    \n",
    "    y_pred = model.predict([x_pred,x_pred_mask,x_pred_pos])\n",
    "    \n",
    "    idx = np.argsort(-y_pred[0,:,0])\n",
    "    pred_idx = np.where(y_pred[0,np.argsort(-y_pred[0,:,0])]>=0.5)\n",
    "    scores = y_pred[0,idx]\n",
    "    res = np.array(x_pred_pos)[0,:,idx[pred_idx[0]]]\n",
    "    text = tokenizer.decode(list(tf.cast(x_pred[0,:],dtype='int32')))\n",
    "    \n",
    "    keyphrases = collections.defaultdict(float)\n",
    "    for i in range(res.shape[0]):\n",
    "        phrase = res[i,:]\n",
    "        phrase = tokenizer.decode(list(tf.cast(x_pred[0,phrase[0]:(phrase[1]+1)],dtype='int32')))\n",
    "        keyphrases[phrase] = max(keyphrases[phrase],scores[i][0])\n",
    "    \n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"D:/Word embedding/bert/assets/vocab.txt\"\n",
    "tokenizer = BertWordPieceTokenizer(vocab, lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_kp depends on the data that was used to \n",
    "max_len = 512\n",
    "max_kp = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# use model configuration according to training \n",
    "tp_model = ext.get_model(max_len,max_kp,36,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15bbf793108>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_model.load_weights('./checkpoints/inspec_final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''AI in healthcare is often used for classification, whether to automate initial evaluation of a CT scan or EKG or to identify high-risk patients for population health. The breadth of applications is rapidly increasing. As an example, AI is being applied to the high-cost problem of dosage issuesâ€”where findings suggested that AI could save $16 billion. In 2016, a groundbreaking study in California found that a mathematical formula developed with the help of AI correctly determined the accurate dose of immunosuppressant drugs to give to organ patients. Artificial intelligence is assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer. There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called \"Hanover\". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. \n",
    "One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers. Another study is using artificial intelligence to try to monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions. One study was done with transfer learning, the machine performed a diagnosis similarly to a well-trained ophthalmologist, and could generate a decision within 30 seconds on whether or not the patient should be referred for treatment, with more than 95% accuracy.\n",
    "According to CNN, a recent study by surgeons at the Children's National Medical Center in Washington successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels). Watson has struggled to achieve success and adoption in healthcare.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'moment': 0.7169198,\n",
       "             'pigs bowel': 0.6988427,\n",
       "             'robot': 0.6974003,\n",
       "             'patient': 0.6962047,\n",
       "             'using': 0.69249254,\n",
       "             'accuracy': 0.6915171,\n",
       "             'multiple highrisk patients': 0.6894527,\n",
       "             'team': 0.6885516,\n",
       "             'transfer': 0.6859856,\n",
       "             'help': 0.68410856,\n",
       "             'mathematical formula': 0.6839912,\n",
       "             'study': 0.68347615,\n",
       "             'dosage issues': 0.68192583,\n",
       "             'research': 0.6791351,\n",
       "             'goal': 0.6772472,\n",
       "             'artificial intelligence computer': 0.676593,\n",
       "             'vaccines': 0.674519,\n",
       "             'learning': 0.6727299,\n",
       "             'seconds': 0.6722069,\n",
       "             'surgery': 0.6712767,\n",
       "             'papers': 0.6707486,\n",
       "             'drugs': 0.6706586,\n",
       "             'data': 0.6692536,\n",
       "             'softtissue surgery': 0.66810095,\n",
       "             'immunosuppressant drugs': 0.6666205,\n",
       "             'project': 0.66652536,\n",
       "             'asking': 0.6650532,\n",
       "             'human intelligence': 0.6649072,\n",
       "             'success': 0.6642753,\n",
       "             'cancer': 0.6635381,\n",
       "             'decision': 0.66306096,\n",
       "             'welltrained ophthalmologist': 0.66236514,\n",
       "             'detail': 0.6623132,\n",
       "             'great amount': 0.6621608,\n",
       "             'doctors': 0.66187835,\n",
       "             'autonomous robot': 0.6614316,\n",
       "             'stitching': 0.6574754,\n",
       "             'highcost problem': 0.6574426,\n",
       "             'highrisk patients': 0.65625787,\n",
       "             'applications': 0.65531933,\n",
       "             'machine': 0.6549607,\n",
       "             'findings': 0.6548914,\n",
       "             'fighting': 0.6511939,\n",
       "             'classification': 0.64596325,\n",
       "             'relating': 0.64579654,\n",
       "             'increasing': 0.64541656,\n",
       "             'diagnosis': 0.64440626,\n",
       "             'treatment': 0.64393425,\n",
       "             'fatal cancer': 0.6437993,\n",
       "             'live doctor': 0.642022,\n",
       "             'healthcare': 0.64152384,\n",
       "             'surgeons': 0.6409944,\n",
       "             'artificial intelligence': 0.6374178,\n",
       "             'right drugs': 0.63577276,\n",
       "             'skin cancers': 0.62907374,\n",
       "             'working': 0.62657505,\n",
       "             'patient numerous questions': 0.6251348,\n",
       "             'many options': 0.62505776,\n",
       "             'groundbreaking study': 0.6239155,\n",
       "             'population health': 0.62290424,\n",
       "             'interactions': 0.62095517,\n",
       "             'decades': 0.6205996,\n",
       "             'trained doctors': 0.61959034,\n",
       "             'open surgery': 0.61798996,\n",
       "             'initial evaluation': 0.61779517,\n",
       "             'breadth': 0.6092442,\n",
       "             'combinations': 0.60811424,\n",
       "             'adoption': 0.60789025,\n",
       "             'recent study': 0.6033503,\n",
       "             'making': 0.5922779,\n",
       "             'human surgeon': 0.58240634,\n",
       "             'assisting': 0.5818689,\n",
       "             'identifying': 0.57691807,\n",
       "             'right treatments': 0.5360505,\n",
       "             'predict': 0.5285091,\n",
       "             'accurate dose': 0.50973946})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_kps(text,tp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
